# -*- coding: utf-8 -*-
# -*- coding: utf-8 -*-
"""
Created on Fri Jul 19 21:57:37 2024

@author: lenovo
"""
#%%å¯¼å…¥
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import torch
import torch.optim as optim
from lightgbm import LGBMClassifier
import sklearn
import random
import pandas as pd
import os
import scipy.io as sio
from scipy import sparse


# NOTE: Î˜ä¸­çš„ç½®ä¿¡åº¦æŒ‡çš„æ˜¯åˆ†ç±»å™¨çš„å‡†ç¡®ç‡, å…¶å½¢çŠ¶ä¸º(c Ã— k);
# Gj(x_i)ä¸­çš„å…ƒç´ æ˜¯åˆ†ç±»å™¨Gjå°†x_ié¢„æµ‹ä¸ºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡
#%%å‡½æ•°
def Train(X, y, k, deep = 10):
    """
    è®­ç»ƒkä¸ªåŸºåˆ†ç±»å™¨

    å‚æ•°:
    X: ç‰¹å¾æ•°æ®
    y: æ ‡ç­¾æ•°æ®
    k: åŸºåˆ†ç±»å™¨çš„ä¸ªæ•°
    deep: åŸºåˆ†ç±»å™¨çš„æœ€å¤§æ·±åº¦

    è¿”å›:
    classifiers: è®­ç»ƒå¥½çš„åŸºåˆ†ç±»å™¨åˆ—è¡¨
    G: é¢„æµ‹ç»“æœçŸ©é˜µ
    accuracies: åŸºåˆ†ç±»å™¨çš„å‡†ç¡®ç‡åˆ—è¡¨
    """

    #deepæ˜¯æœ€å¤§æ·±åº¦
    c = len(np.unique(y))  # ç±»åˆ«æ•°ç›®
    n = len(X)

    # åˆå§‹åŒ–kä¸ªåˆ†ç±»å™¨å’ŒçŸ©é˜µG
    classifiers = []
    # NOTE: è®ºæ–‡ä¸­æ²¡æœ‰æåˆ°G
    G = np.zeros((k * c, n))  # åˆå§‹åŒ–GçŸ©é˜µï¼Œkcè¡Œ, nåˆ—, kä¸ªåˆ†ç±»å™¨ï¼Œæ¯ä¸ªåˆ†ç±»å™¨å¯¹åº”cç»´one-hot encoded vector
    # NOTE: Gä¸­æ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯cè¡Œå¯¹åº”ä¸€ä¸ªåˆ†ç±»å™¨å¯¹è¯¥æ ·æœ¬åœ¨æ¯ä¸ªç±»åˆ«ä¸Šçš„é¢„æµ‹æ¦‚ç‡ï¼Œæ€»å…±æœ‰kä¸ªåˆ†ç±»å™¨ï¼Œæ‰€ä»¥æ˜¯k * cè¡Œã€‚
    # Gçš„ç¬¬iåˆ— = [G1(x_i) -> cç»´åˆ—å‘é‡
    #               ,
    #            G2(x_i) -> cç»´åˆ—å‘é‡
    #               ,
    #              ...
    #               ,
    #            Gk(x_i) -> cç»´åˆ—å‘é‡]
    accuracies = []

    # è®­ç»ƒkä¸ªä¸åŒå‚æ•°è®¾ç½®çš„åˆ†ç±»å™¨
    for i in range(k):
        # Part1 Bootstrap Sampling
        # ä½¿ç”¨bootstrapæŠ½æ ·åˆ›å»ºä¸åŒçš„å­æ ·æœ¬
        size = round(len(X)*0.95)       # æŠ½å–å‡ºå åŸå§‹æ•°æ®é›†95%daxå¤§å°çš„å­æ ·æœ¬é›†

        # ä»åŸå§‹æ•°æ®é›†ä¸­æœ‰æ”¾å›åœ°éšæœºæŠ½å– size ä¸ªæ ·æœ¬çš„ç´¢å¼•ï¼Œå½¢æˆå­æ ·æœ¬çš„ç´¢å¼•åˆ—è¡¨bootstrap_indicesã€‚
        # replace = True: è¡¨ç¤ºæŠ½æ ·æ—¶æ˜¯æœ‰æ”¾å›çš„ï¼Œæ„å‘³ç€æ¯æ¬¡æŠ½å–åï¼Œæ ·æœ¬éƒ½ä¼šæ”¾å›æŠ½æ ·æ± ï¼Œå¯èƒ½è¢«å†æ¬¡æŠ½å–åˆ°ã€‚è¿™ä½¿å¾—ç´¢å¼•åˆ—è¡¨ bootstrap_indices ä¸­çš„å…ƒç´ å¯èƒ½é‡å¤ã€‚
        bootstrap_indices = np.random.choice(len(X), size = size, replace = True)

        # ä»åŸå§‹æ•°æ®é›† X å’Œæ ‡ç­¾é›† y ä¸­æå–å¯¹åº”çš„æ ·æœ¬å’Œæ ‡ç­¾ï¼Œæ„æˆæ–°çš„å­æ ·æœ¬é›†ã€‚
        X_bootstrap = X[bootstrap_indices]
        y_bootstrap = y[bootstrap_indices]

        # Part2 Training Classifiers
        # æ ¹æ®içš„ä¸åŒä½™æ•°é€‰æ‹©ä¸åŒçš„åˆ†ç±»å™¨
        classifier = DecisionTreeClassifier(max_depth = deep, random_state=1)

        # è®­ç»ƒåˆ†ç±»å™¨
        classifier.fit(X_bootstrap, y_bootstrap)
        classifiers.append(classifier)

        # é¢„æµ‹ç»“æœ
        y_pred = classifier.predict(X) - 1

        # Part3 Constructing G Matrix
        # cè¡Œ, nåˆ—: classifier.predict_proba(X) äº§ç”Ÿä¸€ä¸ªå½¢çŠ¶ä¸º(ğ‘›,ğ‘)çš„çŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬åœ¨ğ‘ä¸ªç±»åˆ«ä¸Šçš„é¢„æµ‹æ¦‚ç‡ã€‚
        # NOTE: Gä¸­çš„å…ƒç´ æ˜¯æ ·æœ¬åœ¨æ¯ä¸ªç±»åˆ«ä¸Šçš„é¢„æµ‹æ¦‚ç‡
        G[i*c: i*c+c, :] = classifier.predict_proba(X).T

        # è®¡ç®—åˆ†ç±»å™¨çš„å‡†ç¡®ç‡
        accuracy = classifier.score(X, y)
        accuracies.append(accuracy)
        #print(f"åŸºåˆ†ç±»å™¨ {i+1} çš„å‡†ç¡®ç‡ï¼š{accuracy:.2f}")

    g = []
    for j in range(n):
        g_j = G[:, j:j+1].reshape(k, c)
        g.append(g_j)

    g = np.hstack(g)

    # è¿”å›æ‰€æœ‰åŸºåˆ†ç±»å™¨ã€çŸ©é˜µGå’Œå‡†ç¡®ç‡åˆ—è¡¨
    return classifiers, g, accuracies


def predict_g(classifiers, X, c):
    k = len(classifiers)
    n = len(X)      # æ ·æœ¬æ•°

    g = np.zeros((k, n * c))  # åˆå§‹åŒ–GçŸ©é˜µï¼Œkcè¡Œï¼Œnåˆ—

    # å¯¹æ¯ä¸ªåŸºåˆ†ç±»å™¨è®¡ç®—é¢„æµ‹ç»“æœå¹¶æ›´æ–°çŸ©é˜µG
    for i, clf in enumerate(classifiers):
        y_pred = clf.predict(X) - 1
        g[i, :] = clf.predict_proba(X).reshape(1, -1)

    return g


def one_hot(y):
    y = y.astype(int)
    """
    å°†æ ‡ç­¾å‘é‡ y è½¬æ¢æˆ one-hot ç¼–ç çš„çŸ©é˜µ Y

    å‚æ•°ï¼š
    y : numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n,)ï¼Œæ ‡ç­¾å‘é‡ï¼ŒåŒ…å«æ¯ä¸ªæ ·æœ¬çš„ç±»åˆ«æ ‡ç­¾ï¼ˆä»1åˆ°cï¼‰

    è¿”å›ï¼š
    Y : numpyæ•°ç»„ï¼Œå½¢çŠ¶ä¸º (c, n)ï¼Œæ¯ä¸€åˆ—æ˜¯ y çš„ç›¸åº”å…ƒç´ çš„ one-hot ç¼–ç 
    """
    n = len(y)  # æ ·æœ¬æ•°
    c = int(np.max(y))  # ç±»åˆ«æ•°ï¼Œå‡è®¾ç±»åˆ«æ ‡ç­¾ä»1åˆ°c

    Y = np.zeros((c, n))  # NOTE: æ ·æœ¬æ˜¯ä¸€åˆ—ä¸€åˆ—æ”¾ç½®çš„ï¼Œè¡Œè¡¨ç¤ºç±»åˆ«ï¼Œåˆ—è¡¨ç¤ºæ ·æœ¬

    for i in range(n):
        # ç¬¬iä¸ªæ ·æœ¬çš„æ ‡ç­¾æ˜¯y[i]ï¼Œæ ‡ç­¾y[i]å¯¹åº”çš„ç´¢å¼•æ˜¯y[i]-1
        Y[y[i]-1 , i] = 1  # æ ‡ç­¾ä»1å¼€å§‹ï¼Œæ‰€ä»¥æ ‡ç­¾y[i]å¯¹åº”çš„ç´¢å¼•æ˜¯y[i]-1

    return Y


def init_Theta(accuracies,c):
    """ Thetaçš„å½¢çŠ¶æ˜¯c Ã— kçš„ï¼Œæ¯ä¸ªclassifierå¯¹åº”ä¸€åˆ—ï¼ˆå…±kåˆ—ï¼‰ï¼Œæ¯ä¸ªç±»å¯¹åº”ä¸€è¡Œï¼Œè¡¨ç¤ºä¸€ä¸ªclassifierå°†æ ·æœ¬åˆ†åˆ°è¯¥ç±»çš„ç½®ä¿¡åº¦ï¼Œåˆå§‹åŒ–æ—¶ï¼ŒæŠŠä¸€ä¸ªåˆ†ç±»å™¨å°†æ ·æœ¬åˆ†åˆ°æ¯ä¸ªç±»çš„ç½®ä¿¡åº¦å…¨éƒ¨åˆå§‹åŒ–æˆè¯¥åˆ†ç±»å™¨çš„å‡†ç¡®ç‡ """
    # å°†å‘é‡ä¸­çš„æ¯ä¸ªå…ƒç´ å¤åˆ¶cé
    # replicated = np.repeat(accuracies, c)

    # å°†accuracies(å¯¹åº”äºä¸€è¡Œkä¸ªåˆ†ç±»å™¨çš„å‡†ç¡®ç‡)æ•´ä½“å¤åˆ¶céå¹¶ç«–ç€å †å 
    stacked = np.tile(accuracies, (c, 1))

    return stacked


def compute_S_I_Theta_g_1(Theta, g):
    if not isinstance(g, torch.Tensor):
        g = torch.tensor(g, dtype=torch.float)
    Theta_g = torch.matmul(Theta, g)
    Theta_g = torch.split(Theta_g, c, dim=1)
    I_Theta_g_1 = torch.hstack(
                    [torch.diagonal(element).reshape(-1, 1) for element in Theta_g]
                  )
    S_I_Theta_g_1 = torch.softmax(I_Theta_g_1, dim=0)

    return S_I_Theta_g_1


def compute_loss(Theta, g, Y, gamma, alpha = 100):
    # Thetaçš„å½¢çŠ¶æ˜¯c Ã— kçš„
    c = Theta.size()[0]
    n = int(g.shape[1] / c)

    #è½¬åŒ–æˆå¼ é‡
    g = torch.tensor(g, dtype=torch.float)
    Y = torch.tensor(Y, dtype=torch.float)


    S_I_Theta_g_1 = compute_S_I_Theta_g_1(Theta, g)

    # åˆå§‹åŒ–marginå’Œcross-entropy loss
    M = 0
    C = 0

    for idx in range(n):
        M += torch.matmul(Y[:, idx:idx+1].T, S_I_Theta_g_1[:, idx:idx+1])
        C -= torch.matmul(Y[:, idx:idx+1].T, torch.log(S_I_Theta_g_1[:, idx:idx+1]))

    M -= 1 / alpha * torch.sum(
                     torch.logsumexp(alpha * (S_I_Theta_g_1 - Y * S_I_Theta_g_1),
                                     dim=0),
                     )

    L = (C - gamma * M) / n

    return L


def generate_ring_data(num_samples, inner_radius, outer_radius, noise_std):
    # ç”Ÿæˆç¬¬ä¸€ä¸ªæœˆç‰™æ•°æ®ï¼ˆä¸ŠåŠåœ†ï¼‰
    theta_first = np.linspace(0, np.pi, num_samples)
    x_first = inner_radius * np.cos(theta_first) + noise_std * np.random.randn(num_samples)
    y_first = inner_radius * np.sin(theta_first) + noise_std * np.random.randn(num_samples)

    # ç”Ÿæˆç¬¬äºŒä¸ªæœˆç‰™æ•°æ®ï¼ˆä¸‹åŠåœ†ï¼Œåç§»ï¼‰
    theta_second = np.linspace(0, np.pi, num_samples)
    x_second = outer_radius * np.cos(theta_second) + inner_radius
    x_second += noise_std * np.random.randn(num_samples)
    y_second = -outer_radius * np.sin(theta_second) + noise_std * np.random.randn(num_samples)

    # åˆå¹¶ä¸¤ä¸ªæœˆç‰™æ•°æ®
    x = np.concatenate((x_first, x_second))
    y = np.concatenate((y_first, y_second))

    # åˆ›å»ºæ ‡ç­¾å‘é‡ yï¼Œç¬¬ä¸€ä¸ªæœˆç‰™ä¸ºæ ‡ç­¾ 1ï¼Œç¬¬äºŒä¸ªæœˆç‰™ä¸ºæ ‡ç­¾ 2
    labels = np.ones(num_samples * 2)
    labels[num_samples:] = 2

    return x, y, labels


def loadmat(path, to_dense = True):
    data = sio.loadmat(path)
    X = data["X"]
    y_true = data["Y"].astype(np.int32).reshape(-1)

    if sparse.isspmatrix(X) and to_dense:
        X = X.toarray()

    N, dim, c_true = X.shape[0], X.shape[1], len(np.unique(y_true))
    return X, y_true, N, dim, c_true


def compute_classifiers_Y_G_accracies(X_train, y_train, k, deep):
    classifiers, g, accuracies = Train(X_train, y_train, k, deep=deep)
    Y = one_hot(y_train)
    # g = torch.tensor(g, dtype=torch.float)

    return classifiers, Y, g, accuracies


def compute_results(X, labels,
                    y_train,
                    X_test, y_test,
                    classifiers, Theta,
                    g, c):
    g_test = predict_g(classifiers, X_test, c)
    g_test = torch.tensor(g_test, dtype=torch.float)

    y_dataset_test= torch.argmax(compute_S_I_Theta_g_1(Theta, g_test), dim=0).numpy() + 1
    acc_dataset_test = accuracy_score(y_dataset_test, y_test)

    y_dataset_train = torch.argmax(compute_S_I_Theta_g_1(Theta, g), dim=0).numpy() + 1
    acc_dataset_train = accuracy_score(y_dataset_train, y_train)

    g_all = predict_g(classifiers, X, c)
    g_all = torch.tensor(g_all,dtype=torch.float)
    y_dataset_all= torch.argmax(compute_S_I_Theta_g_1(Theta, g_all), dim=0).numpy() + 1
    acc_dataset_all = accuracy_score(y_dataset_all, labels)

    return y_dataset_test, acc_dataset_test, y_dataset_train, acc_dataset_train, y_dataset_all, acc_dataset_all




SEED = 1
# pythonå†…ç½®ç§å­
random.seed(SEED)
# torchç§å­
torch.manual_seed(1)
# numpyç§å­
np.random.seed(1)
# sklearnç§å­
sklearn.random.seed(SEED)

deep = 9
# è·å–æ‰€æœ‰.matæ–‡ä»¶
data_path = r'data'
mat_files = ['BASEHOCK.mat', 'breast_uni.mat', 'chess.mat', 'iris.mat', 'jaffe.mat', 'pathbased.mat', 'RELATHE.mat', 'wine.mat']

mat_files = [os.path.join(data_path, f) for f in mat_files]

# åˆ›å»ºç»“æœDataFrame
columns = ['wrf_train', 'wrf_test', 'wrf_all',
           'rf50_train', 'rf50_test', 'rf50_all',
           'rf100_train', 'rf100_test', 'rf100_all',
           'svc_train', 'svc_test', 'svc_all',
           'xgb_train', 'xgb_test', 'xgb_all',
           'lgbm_train', 'lgbm_test', 'lgbm_all']
results = pd.DataFrame(columns = columns)

# å¾ªç¯å¤„ç†æ¯ä¸ªæ•°æ®é›†
for mat_file in mat_files:
    dataset_name = os.path.splitext(os.path.basename(mat_file))[0]
    print(f"Processing dataset: {dataset_name}")

    try:
        # åŠ è½½æ•°æ®
        X, labels, num, dim, c = loadmat(mat_file)
        X = X.astype(np.float32)
        if min(labels) == 0:
            labels = labels + 1
        if min(labels) == -1:
            labels = (labels + 1) / 2 + 1
        c = int(max(labels))
        X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size = 0.2, random_state = 42)


        # Part1 WRF
        k = 10
        gamma = 10
        n = 1000
        loss_array = np.ones(n)

        classifiers, Y, g, accuracies = compute_classifiers_Y_G_accracies(X_train, y_train, k, deep)

        # è®¡ç®—Theta
        Theta = init_Theta(accuracies, c)
        Theta = torch.tensor(Theta, requires_grad=True, dtype=torch.float)

        # å®šä¹‰ä¼˜åŒ–å™¨
        optimizer = optim.SGD([Theta], lr=2)

        # è®­ç»ƒè¿‡ç¨‹
        for epoch in range(n):
            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
            loss = compute_loss(Theta, g, Y, gamma)  # è®¡ç®—æŸå¤±
            # ä½¿ç”¨.item()æ–¹æ³•ä»åŒ…å«å•ä¸ªå…ƒç´ çš„æ•°ç»„ä¸­æå–å‡ºè¯¥å…ƒç´ çš„æ ‡é‡å€¼ã€‚
            loss_array[epoch] = loss.detach().numpy().item()
            loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()  # æ›´æ–°å‚æ•°

            if (epoch + 1) % 100 == 0:
                print(f'Epoch [{epoch+1}/{n}], Loss: {loss.item()}')

        # è·å–ä¼˜åŒ–åçš„å‚æ•°å€¼
        optimal_Theta = Theta.detach().numpy()

        y_wrf_test, acc_wrf_test, y_wrf_train, acc_wrf_train, y_wrf_all, acc_wrf_all = compute_results(X, labels, y_train, X_test, y_test, classifiers, Theta, g, c)


        # Part2 RF50
        k_rf = 50

        classifiers, Y, g, accuracies = compute_classifiers_Y_G_accracies(X_train, y_train, k_rf, deep)

        # è®¡ç®—Theta
        Theta = init_Theta(accuracies, c)
        Theta = torch.tensor(Theta, requires_grad=True, dtype=torch.float)

        y_rf50_test, acc_rf50_test, y_rf50_train, acc_rf50_train, y_rf50_all, acc_rf50_all = compute_results(X, labels, y_train, X_test, y_test, classifiers, Theta, g, c)


        # Part3 RF100
        k_rf = 100

        classifiers, Y, g, accuracies = compute_classifiers_Y_G_accracies(X_train, y_train, k_rf, deep)

        # è®¡ç®—Theta
        Theta = init_Theta(accuracies, c)
        Theta = torch.tensor(Theta, requires_grad=True, dtype=torch.float)

        y_rf100_test, acc_rf100_test, y_rf100_train, acc_rf100_train, y_rf100_all, acc_rf100_all = compute_results(X, labels, y_train, X_test, y_test, classifiers, Theta, g, c)


        # SVC
        svm_clf = SVC(kernel = 'rbf', C = 1.0, gamma = 'scale', decision_function_shape = 'ovr')

        # è®­ç»ƒæ¨¡å‹
        svm_clf.fit(X_train, y_train)

        # é¢„æµ‹
        y_svc_test = svm_clf.predict(X_test)
        # è®¡ç®—å‡†ç¡®ç‡
        acc_svc_test = accuracy_score(y_test, y_svc_test)
        y_svc_train = svm_clf.predict(X_train)
        acc_svc_train = accuracy_score(y_train, y_svc_train)
        y_svc_all = svm_clf.predict(X)
        acc_svc_all = accuracy_score(labels, y_svc_all)


        # xgboost
        xgb_clf = XGBClassifier(max_depth = deep, learning_rate = 0.2, n_estimators = k, objective = 'multi:softmax',  # æˆ–è€… 'multi:softprob'
            num_class = c)

        # è®­ç»ƒæ¨¡å‹
        xgb_clf.fit(X_train, y_train-1)

        # é¢„æµ‹
        y_xgb_test = xgb_clf.predict(X_test)
        # è®¡ç®—å‡†ç¡®ç‡
        acc_xgb_test = accuracy_score(y_test, y_xgb_test+1)
        y_xgb_train = xgb_clf.predict(X_train)
        # è®¡ç®—å‡†ç¡®ç‡
        acc_xgb_train = accuracy_score(y_train, y_xgb_train+1)
        y_xgb_all = xgb_clf.predict(X)
        # è®¡ç®—å‡†ç¡®ç‡
        acc_xgb_all = accuracy_score(labels, y_xgb_all+1)


        # lgbm
        lgbm_clf = LGBMClassifier(max_depth = deep, learning_rate = 0.225, n_estimators = k, objective = 'multiclass', num_class = c)

        # è®­ç»ƒæ¨¡å‹
        lgbm_clf.fit(X_train, y_train-1)

        # é¢„æµ‹
        y_lgbm_test = lgbm_clf.predict(X_test)
        # è®¡ç®—æµ‹è¯•é›†å‡†ç¡®ç‡
        acc_lgbm_test = accuracy_score(y_test, y_lgbm_test+1)

        # é¢„æµ‹è®­ç»ƒé›†
        y_lgbm_train = lgbm_clf.predict(X_train)
        # è®¡ç®—è®­ç»ƒé›†å‡†ç¡®ç‡
        acc_lgbm_train = accuracy_score(y_train, y_lgbm_train+1)

        # é¢„æµ‹æ‰€æœ‰æ•°æ®
        y_lgbm_all = lgbm_clf.predict(X)
        # è®¡ç®—æ‰€æœ‰æ•°æ®çš„å‡†ç¡®ç‡
        acc_lgbm_all = accuracy_score(labels, y_lgbm_all+1)


            # å­˜å‚¨ç»“æœ
        results.loc[dataset_name] = [
            acc_wrf_train, acc_wrf_test, acc_wrf_all,
            acc_rf50_train, acc_rf50_test, acc_rf50_all,
            acc_rf100_train, acc_rf100_test, acc_rf100_all,
            acc_svc_train, acc_svc_test, acc_svc_all,
            acc_xgb_train, acc_xgb_test, acc_xgb_all,
            acc_lgbm_train, acc_lgbm_test, acc_lgbm_all
        ]
    except Exception as e:
        print(f"######## Error processing dataset {dataset_name}: {str(e)} ########")
        continue

# ä¿å­˜ç»“æœåˆ°CSV
results.to_csv(r"results.csv")
print("Results saved to algorithm_comparison_results.csv")














# è¾¹ç•Œ
# from matplotlib.colors import ListedColormap
# def boundary(clf,X,labels,Theta = np.array([[1,2],[3,4]]),c = 2):
#     cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])
#     cmap_bold = ListedColormap(['#FF0000', '#00FF00'])
#     h = .1
#     x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
#     y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
#                          np.arange(y_min, y_max, h))
#     data = np.c_[xx.ravel(), yy.ravel()]
#     if sum(sum(Theta)) =  = 10:
#         Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
#         Z = Z + 1  # è°ƒæ•´é¢„æµ‹æ ‡ç­¾èŒƒå›´åˆ°1, 2
#         Z = Z.reshape(xx.shape)

#         plt.figure(figsize = (10, 6))
#         plt.contourf(xx, yy, Z, cmap = cmap_light, alpha = 0.8)
#         plt.scatter(X[:, 0], X[:, 1], c = labels, cmap = cmap_bold, edgecolor = 'k', s = 20)
#         plt.xlim(xx.min(), xx.max())
#         plt.ylim(yy.min(), yy.max())
#         plt.title("XGBoost Decision Boundary")
#         plt.xlabel('Feature 1')
#         plt.ylabel('Feature 2')
#         plt.show()
#     else:
#         G = predict_g(clf, data , c)
#         G = torch.tensor(G,dtype=torch.float)
#         Z= torch.argmax(torch.matmul(Theta,G), dim = 0).numpy()+1
#         Z = Z.reshape(xx.shape)
#         plt.figure(figsize = (10, 6))
#         plt.contourf(xx, yy, Z, cmap = cmap_light, alpha = 0.8)
#         plt.scatter(X[:, 0], X[:, 1], c = labels, cmap = cmap_bold, edgecolor = 'k', s = 20)
#         plt.xlim(xx.min(), xx.max())
#         plt.ylim(yy.min(), yy.max())
#         plt.title("XGBoost Decision Boundary")
#         plt.xlabel('Feature 1')
#         plt.ylabel('Feature 2')
#         plt.show()
#     return (xx,yy,Z)

#boundary(classifiers,X,labels,Theta,c = 2)
